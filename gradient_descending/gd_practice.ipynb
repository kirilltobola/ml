{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snxSUWXD7q_p"
   },
   "source": [
    "## Градиентный спуск — практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-8OLsAV7wrQ"
   },
   "source": [
    "Стохастический градиентый спуск сходится быстрее, чем полный, хотя и менее стабильно. В этом задании вам предлагается реализовать стохастический градиентный спуск и сравнить его с точным вычислением весов линейной модели по скорости работы и значению метрики качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IgQyWw5o7Nej"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGD1wQgMruJw"
   },
   "source": [
    "### Задание 0\n",
    "\n",
    "Реализуйте класс ```LinearRegressionSGD``` c обучением и и применением линейной регрессии, построенной с помощью стохастического градиентного спуска, с заданным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QZxdV27R9-uc"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import random\n",
    "\n",
    "class LinearRegressionSGD(BaseEstimator):\n",
    "    def __init__(self, epsilon=1e-4, max_steps=100, w0=None, alpha=1e-6):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        l, d = X.shape\n",
    "        \n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.zeros(d)\n",
    "        \n",
    "        self.w = self.w0\n",
    "        for step in range(self.max_steps):\n",
    "            ## На каждом шаге градиентного спуска веса необходимо добавлять в w_history\n",
    "            self.w_history.append(self.w)\n",
    "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
    "            \n",
    "            if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
    "                break\n",
    "            self.w = w_new\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        l, d = X.shape\n",
    "        y_pred = []\n",
    "        for i in range(l):\n",
    "            y_pred.append(np.dot(X[i], self.w))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        l, d = X.shape\n",
    "        i = np.random.randint(d)\n",
    "        gradient = np.zeros(d)\n",
    "        \n",
    "        for j in range(l):\n",
    "            gradient[i] += 2 / l * X[j][i] * (np.dot(X[j], self.w) - y[j])\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNOm9-bXpdT3"
   },
   "source": [
    "Проверять работу мы будем на имеющемся в sklearn наборе данных boston: в нём нужно по информации о доме предсказать его стоимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c24JCwes9-pe"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eIJwWnInXnr"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Метрикой качества в нашей задаче будет MAPE - Mean Absolute Percentage Error. Реализуйте её с заданным интерфейсом и вычислите \n",
    "```MAPE(y_test, y_0)```, где ```y_0 = (mean(y_test), mean(y_test), ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "znoDavxyuLsi"
   },
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        y_true: np.array (l)\n",
    "        y_pred: np.array (l)\n",
    "        ---\n",
    "        output: float [0, +inf)\n",
    "    \"\"\"\n",
    "    n = y_true.shape[0]\n",
    "    M = 0\n",
    "    for i, true_value in enumerate(y_true):\n",
    "        M += abs(\n",
    "            (true_value - y_pred[i]) / true_value\n",
    "        )\n",
    "    return M / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e6mTAykeojwp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37415882976840953"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0 = np.array([np.mean(y_test) for i in range(y_test.shape[0])])\n",
    "MAPE(y_test, y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nNy2ITxuMKf"
   },
   "source": [
    "### Задание 2 \n",
    "\n",
    "Обучите ```LinearRegressionSGD``` с базовыми параметрами на тренировочном наборе данных (```X_train```, ```y_train```), сделайте предсказание на тестовых данных ```X_test```, записав результат в переменную ```y_pred_sgd```  и вычислите ошибку MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7BIHwAwUvB-N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999564046844806"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = LinearRegressionSGD()\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "\n",
    "MAPE(y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWappMdMtIPV"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Вычислите веса по точной формуле, используя ```X_train``` и ```y_train```; предскажите с их помощью целевую переменную на ```X_test```, записав результат в переменную ```y_pred_lr``` и вычислите ошибку MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wjMUlPje9-k0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1895313481637619"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.linalg.inv(\n",
    "    np.transpose(X_train) @ X_train\n",
    ") @ np.transpose(X_train) @ y_train\n",
    "\n",
    "y_pred_lr = []\n",
    "for i in X_test:\n",
    "    y_pred_lr.append(i @ w)\n",
    "\n",
    "MAPE(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZFaUn7yx04u"
   },
   "source": [
    "### Задание 4\n",
    "До этого мы использовали модели, в которых не было штрафа за величину весов ```w```. Это может привести к неустойчивости модели и переобучению. Чтобы избежать этих эффектов, предлагается добавить к оптимизируемому функционалу L2-норму весов; таким образом, будем решать задачу гребневой регрессии, Ridge:\n",
    "\n",
    "$$ \\frac{1}{l}(Xw-y)^T(Xw-y) +\\gamma||w||_2 \\rightarrow \\min_{w}. $$\n",
    "\n",
    "Реализуйте обучение такой модели в матричном виде с помощью стохастического градиентного спуска. Класс должен совпадать по набору реализованных функций с ```LinearRegressionVectorized```, разница будет лишь в параметре $\\gamma$, отвечающем за степень регуляризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TEXqBqmGxWDz"
   },
   "outputs": [],
   "source": [
    "class RidgeSGD(BaseEstimator):\n",
    "    def __init__(self, epsilon=1e-4, max_steps=1000, w0=None, alpha=1e-2, gamma=0):\n",
    "        \"\"\"\n",
    "        epsilon: разница для нормы изменения весов \n",
    "        max_steps: максимальное количество шагов в градиентном спуске\n",
    "        w0: np.array (d,) - начальные веса\n",
    "        alpha: шаг обучения\n",
    "        gamma: коэффициент регуляризации\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        ---\n",
    "        output: np.array (l)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array (l, d)\n",
    "        y: np.array (l)\n",
    "        ---\n",
    "        output: np.array (d)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t9rqXFu8Pq6"
   },
   "source": [
    "Так же, как и в основном задании, обучите модель с базовыми параметрами на тренировочных данных и сделайте прогноз y_pred_ridge. Выведите значение MAPE(y_test, y_pred_ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6A2hak_A8QPO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML Mag W04 GradientDescent practice",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
